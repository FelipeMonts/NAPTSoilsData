library(dplyr)
out.1 <- extract_tables(NAPT.paths[1]) ;
out.1
str(out.1)
length(out.1)
out.1[[1]]
str(out.1[[1]])
soil.names<-out.1[[1]][1,]
soil.names
soil_names<-paste(soil.names,analysis.names,sep="_");
a
soil.names<-out.1[[1]][1,]
analysis.names<-c("Soil" , "Units" , "n", rep(c("Median", "MAD"),5))
soil_names<-paste(soil.names,analysis.names,sep="_");
Results.data.1<-out.1[[length(out.1)]];
str(Results.data.1)
Results.data.1
names(Results.data.1)<-soil_names
Results.data.1
Results.data.1<-t(out.1[[length(out.1)]]);
Results.data.1
row.names(Results.data.1)<-soil_names
Results.data.1
soil_names<-paste(soil.names,analysis.names,sep="_");
soil_names
# Preliminaries
rm(list = ls())
setwd("C:/Felipe/LaserDifractionSoilTextureAnalysis/NAPTSoilsData") ;
# httr is a package for downloading html
library(httr)
# A package for manipulating strings
library(stringr)
# Lets start by downloading an example web page:
url <- "http://www.naptprogram.org/lab-results/program-archive"
# We start by using the httr package to download the source html
page <- httr::GET(url)
# To get at the actual content of the page, we use the content() function:
page_content <- httr::content(page, "text")
# As we can see, this produces a great deal of information
str(page)
# Now lets print it out
cat(page_content)
url.NAPT<-"http://www.naptprogram.org/lab-results/program-archive"
NAPT.page<-httr::GET(url.NAPT)
library(XML)
library(rvest)
library(magrittr)
scraping_NAPT <- read_html ("http://www.naptprogram.org/lab-results")
Lab_Results   <- scraping_NAPT %>%
html_nodes ("td>a")
length(Lab_Results)
Node.names.scraping_NAPT<-html_text(Lab_Results) ;
str(Node.names.scraping_NAPT)
Soils.NAPT<-which(Node.names.scraping_NAPT == "Soil") ;
length(Lab_Results[Soils.NAPT])
head(Lab_Results[Soils.NAPT])
Soil.NAPT.paths<-strsplit(as.character(Lab_Results[Soils.NAPT]), split='"') ;
str(Soil.NAPT.paths)
NAPT.pdfs.1<-sapply(Soil.NAPT.paths,'[',2) ;
NAPT.paths<-paste0('http://www.naptprogram.org/',NAPT.pdfs.1) ;
library(tabulizer)
library(dplyr)
out.1 <- extract_tables(NAPT.paths[1]) ;
str(out.1)
str(out.1)
length(out.1)
out.1[[1]]
str(out.1[[1]])
soil.names<-out.1[[1]][1,]
soil.names
analysis.names<-c("Soil" , "Units" , "n", rep(c("Median", "MAD"),5))
analysis.names
soil.names<-out.1[[1]][1,c(1,2,3,4,5,7,8,10,11,13,14,16,17)]
soil.names
soil.names<-out.1[[1]][1,c(1,2,3,4,5,7,8,10,11,13,14,16,17)]
analysis.names<-c("Soil" , "Units" , "n", rep(c("Median", "MAD"),5))
soil_names<-paste(soil.names,analysis.names,sep="_");
soil_names
Results.data.1<-t(out.1[[length(out.1)]]);
row.names(Results.data.1)<-soil_names
str(Results.data.1)
Results.data.1
pdf.data.1<-t(out.1[[length(out.1)]]);
row.names(pdf.data.1)<-soil_names
str(Results.data.1)
greg('Particle$',pdf.data.1[2,])
grep('Particle$',pdf.data.1[2,])
grep('Particle.*',pdf.data.1[1,])
grep('Particle.*',pdf.data.1[1,])+3
grep('Particle.*',pdf.data.1[1,])
min(grep('Particle.*',pdf.data.1[1,]))
max(grep('Particle.*',pdf.data.1[1,])+3)
select.columns<-seq(min(grep('Particle.*',pdf.data.1[1,])),max(grep('Particle.*',pdf.data.1[1,])+3)) ;
select.columns
grep('Particle.*',pdf.data.1[1,])
grep('Sand.*',pdf.data.1[1,])
grep('Silt.*',pdf.data.1[1,])
grep('Clay.*',pdf.data.1[1,])
c(grep('Sand.*',pdf.data.1[1,],grep('Silt.*',pdf.data.1[1,]),grep('Clay.*',pdf.data.1[1,]))
c(grep('Sand.*',pdf.data.1[1,]),grep('Silt.*',pdf.data.1[1,]),grep('Clay.*',pdf.data.1[1,]))
grep('Sand.*',pdf.data.1[1,])
grep('Silt.*',pdf.data.1[1,])
grep('Clay.*',pdf.data.1[1,])
c(grep('Sand.*',pdf.data.1[1,]),grep('Silt.*',pdf.data.1[1,]),grep('Clay.*',pdf.data.1[1,]))
sort(c(grep('Sand.*',pdf.data.1[1,]),grep('Silt.*',pdf.data.1[1,]),grep('Clay.*',pdf.data.1[1,])))
select.columns<-sort(c(grep('Sand.*',pdf.data.1[1,]),grep('Silt.*',pdf.data.1[1,]),grep('Clay.*',pdf.data.1[1,])));
Results.data.1<-pdf.data.1[,select.columns]
Results.data.1
Results.data.all<-Results.data.1
soil.names<-out[[1]][1,c(1,2,3,4,5,7,8,10,11,13,14,16,17)]
soil_names<-paste(soil.names,analysis.names,sep="_");
out<-extract_tables(NAPT.paths[2]) ;
str(out)
length(out)
out[[1]]
str(out[[1]])
soil.names<-out[[1]][1,c(1,2,3,4,5,7,8,10,11,13,14,16,17)]
soil_names<-paste(soil.names,analysis.names,sep="_");
pdf.data<-t(out[[length(out)]]);
pdf.data<-t(out[[length(out)]]);
row.names(pdf.data)<-soil_names
str(pdf.data)
select.columns<-sort(c(grep('Sand.*',pdf.data[1,]),grep('Silt.*',pdf.data[1,]),grep('Clay.*',pdf.data[1,])));
select.columns
Results.data<-pdf.data[,select.columns]
Results.data
soil.names<-out[[1]][1
,]
out[[1]]
Results.data<-pdf.data[,select.columns]
Results.data
out[[1]][1,c(1,2,3)]
grep("Soil .*",out[[1]][1,])
soil.names<-out[[1]][1,grep("Soil .*",out[[1]][1,])]
oil.names
soil.names
soil_names
soil.names
rep(c("Median", "MAD"),5)
paste(c("Median"),soil.names,sep='_')
paste(c("MAD"),soil.names,sep='_')
paste(soil.names,c("Median"),sep='_')
paste(soil.names,c("MAD"),sep='_')
c(paste(soil.names,c("Median"),sep='_'),paste(soil.names,c("MAD"),sep='_'))
seq(1,2*length(soil.names)-1,2)
c(paste(soil.names,c("Median"),sep='_'),paste(soil.names,c("MAD"),sep='_'))[seq(1,2*length(soil.names)-1,2)]
c(paste(soil.names,c("Median"),sep='_'),paste(soil.names,c("MAD"),sep='_'))
matrix(paste(soil.names,c("Median"),sep='_'),paste(soil.names,c("MAD"),sep='_'))
matrix(c(paste(soil.names,c("Median"),sep='_'),paste(soil.names,c("MAD"),sep='_')),nrow=lenght(soil.names),nrow=2)
matrix(c(paste(soil.names,c("Median"),sep='_'),paste(soil.names,c("MAD"),sep='_')),nrow=lenght(soil.names),ncol=2)
length(soil.names)
matrix(c(paste(soil.names,c("Median"),sep='_'),paste(soil.names,c("MAD"),sep='_')),nrow=length(soil.names),ncol=2)
unlist<-temp
temp<-matrix(c(paste(soil.names,c("Median"),sep='_'),paste(soil.names,c("MAD"),sep='_')),nrow=length(soil.names),ncol=2)
unlist<-temp
unlist(temp)
as.vector(temp)
as.vector(t(temp))
analysis.names<-c("Analysis" , "Units" , "n", as.vector(t(temp)))
analysis.names
pdf.data<-t(out[[length(out)]]);
row.names(pdf.data)<-soil_names
str(pdf.data)
select.columns<-sort(c(grep('Sand.*',pdf.data[1,]),grep('Silt.*',pdf.data[1,]),grep('Clay.*',pdf.data[1,])));
Results.data<-pdf.data[,select.columns]
Results.data
############# combine the data with the previous pdf data ###########
str(data.1)
soil.names<-out.1[[1]][1,grep("Soil .*",out.1[[1]][1,])]
temp<-matrix(c(paste(soil.names,c("Median"),sep='_'),paste(soil.names,c("MAD"),sep='_')),nrow=length(soil.names),ncol=2)
Soil_names<-as.vector(t(temp))
analysis.names<-c("Analysis" , "Units" , "n", as.vector(t(temp)))
pdf.data.1<-t(out.1[[length(out.1)]]);
row.names(pdf.data.1)<-soil_names
row.names(pdf.data.1)<-Soil_names
row.names(pdf.data.1)<-analysis.names
str(pdf.data.1)
select.columns<-sort(c(grep('Sand.*',pdf.data.1[1,]),grep('Silt.*',pdf.data.1[1,]),grep('Clay.*',pdf.data.1[1,])));
Results.data.1<-pdf.data.1[,select.columns]
Results.data.all<-Results.data.1
Results.data.all
data.1<-Results.data.all
out<-extract_tables(NAPT.paths[2]) ;
str(out)
length(out)
out[[1]]
str(out[[1]])
soil.names<-out[[1]][1,grep("Soil .*",out[[1]][1,])]
temp<-matrix(c(paste(soil.names,c("Median"),sep='_'),paste(soil.names,c("MAD"),sep='_')),nrow=length(soil.names),ncol=2)
Soil_names<-as.vector(t(temp))
analysis.names<-c("Analysis" , "Units" , "n", as.vector(t(temp)))
pdf.data<-t(out[[length(out)]]);
pdf.data<-t(out[[length(out)]]);
row.names(pdf.data)<-analysis.names
str(pdf.data)
select.columns<-sort(c(grep('Sand.*',pdf.data[1,]),grep('Silt.*',pdf.data[1,]),grep('Clay.*',pdf.data[1,])));
Results.data<-pdf.data[,select.columns]
############# combine the data with the previous pdf data ###########
str(data.1)
str(Results.data)
Results.data.all<-rbind(data.1,Results.data)
Results.data.all
str(data.1)
str(Results.data)
str(Results.data.all)
i=1
i
2:lenght(NAPT.paths)
length(NAPT.paths)
NAPT.paths[2:length(NAPT.paths)
]
i=2
data.1<-Results.data.all
i=2
data.1<-Results.data.all
out<-extract_tables(NAPT.paths[i]) ;
str(out)
length(out)
out[[1]]
str(out[[1]])
soil.names<-out[[1]][1,grep("Soil .*",out[[1]][1,])]
temp<-matrix(c(paste(soil.names,c("Median"),sep='_'),paste(soil.names,c("MAD"),sep='_')),nrow=length(soil.names),ncol=2)
Soil_names<-as.vector(t(temp))
analysis.names<-c("Analysis" , "Units" , "n", as.vector(t(temp)))
pdf.data<-t(out[[length(out)]]);
row.names(pdf.data)<-analysis.names
str(pdf.data)
select.columns<-sort(c(grep('Sand.*',pdf.data[1,]),grep('Silt.*',pdf.data[1,]),grep('Clay.*',pdf.data[1,])));
Results.data<-pdf.data[,select.columns]
############# combine the data with the previous pdf data ###########
str(data.1)
str(Results.data)
Results.data.all<-rbind(data.1,Results.data)
str(Results.data.all)
Results.data.all
.libPaths("C:/Felipe/SotwareANDCoding/R_Library/library")  ;
# Preliminaries
rm(list = ls())
setwd("C:/Felipe/LaserDifractionSoilTextureAnalysis/NAPTSoilsData") ;
# httr is a package for downloading html
library(httr)
# A package for manipulating strings
library(stringr)
# Lets start by downloading an example web page:
url <- "http://www.naptprogram.org/lab-results/program-archive"
# We start by using the httr package to download the source html
page <- httr::GET(url)
# As we can see, this produces a great deal of information
str(page)
# To get at the actual content of the page, we use the content() function:
page_content <- httr::content(page, "text")
# Now lets print it out
cat(page_content)
url.NAPT<-"http://www.naptprogram.org/lab-results/program-archive"
NAPT.page<-httr::GET(url.NAPT)
library(XML)
library(rvest)
library(magrittr)
scraping_NAPT <- read_html ("http://www.naptprogram.org/lab-results")
Lab_Results   <- scraping_NAPT %>%
html_nodes ("td>a")
length(Lab_Results)
Node.names.scraping_NAPT<-html_text(Lab_Results) ;
str(Node.names.scraping_NAPT)
Soils.NAPT<-which(Node.names.scraping_NAPT == "Soil") ;
length(Lab_Results[Soils.NAPT])
head(Lab_Results[Soils.NAPT])
Soil.NAPT.paths<-strsplit(as.character(Lab_Results[Soils.NAPT]), split='"') ;
str(Soil.NAPT.paths)
NAPT.pdfs.1<-sapply(Soil.NAPT.paths,'[',2) ;
NAPT.paths<-paste0('http://www.naptprogram.org/',NAPT.pdfs.1) ;
library(tabulizer)
library(dplyr)
out.1 <- extract_tables(NAPT.paths[1]) ;
str(out.1)
length(out.1)
out.1[[1]]
str(out.1[[1]])
soil.names<-out.1[[1]][1,grep("Soil .*",out.1[[1]][1,])]
temp<-matrix(c(paste(soil.names,c("Median"),sep='_'),paste(soil.names,c("MAD"),sep='_')),nrow=length(soil.names),ncol=2)
Soil_names<-as.vector(t(temp))
analysis.names<-c("Analysis" , "Units" , "n", as.vector(t(temp)))
pdf.data.1<-t(out.1[[length(out.1)]]);
row.names(pdf.data.1)<-analysis.names
str(pdf.data.1)
select.columns<-sort(c(grep('Sand.*',pdf.data.1[1,]),grep('Silt.*',pdf.data.1[1,]),grep('Clay.*',pdf.data.1[1,])));
Results.data.1<-pdf.data.1[,select.columns]
Results.data.all<-Results.data.1
i=2
data.1<-Results.data.all
out<-extract_tables(NAPT.paths[i]) ;
str(out)
length(out)
out[[1]]
str(out[[1]])
soil.names<-out[[1]][1,grep("Soil .*",out[[1]][1,])]
temp<-matrix(c(paste(soil.names,c("Median"),sep='_'),paste(soil.names,c("MAD"),sep='_')),nrow=length(soil.names),ncol=2)
Soil_names<-as.vector(t(temp))
analysis.names<-c("Analysis" , "Units" , "n", as.vector(t(temp)))
pdf.data<-t(out[[length(out)]]);
row.names(pdf.data)<-analysis.names
str(pdf.data)
select.columns<-sort(c(grep('Sand.*',pdf.data[1,]),grep('Silt.*',pdf.data[1,]),grep('Clay.*',pdf.data[1,])));
Results.data<-pdf.data[,select.columns]
############# combine the data with the previous pdf data ###########
str(data.1)
str(Results.data)
Results.data.all<-rbind(data.1,Results.data)
str(Results.data.all)
Results.data.all
#  Tell the program where the package libraries are  #####################
.libPaths("C:/Felipe/SotwareANDCoding/R_Library/library")  ;
###### Introduction to Web Scraping #####
# Preliminaries
rm(list = ls())
# Set your working directory to some place you can find
setwd("C:/Felipe/LaserDifractionSoilTextureAnalysis/NAPTSoilsData") ;
# First we will need to install the packages we plan to use for this exercise (
# if they are not already installed on your computer).
# install.packages("httr", dependencies = TRUE)
# install.packages("stringr", dependencies = TRUE)
# httr is a package for downloading html
library(httr)
# A package for manipulating strings
library(stringr)
# Lets start by downloading an example web page:
url <- "http://www.naptprogram.org/lab-results/program-archive"
# We start by using the httr package to download the source html
page <- httr::GET(url)
# As we can see, this produces a great deal of information
str(page)
# To get at the actual content of the page, we use the content() function:
page_content <- httr::content(page, "text")
# Now lets print it out
cat(page_content)
### Web Scraping NAPT Soils ###
url.NAPT<-"http://www.naptprogram.org/lab-results/program-archive"
# we start by using the httr package to download the source html
NAPT.page<-httr::GET(url.NAPT)
#######################################################################################################################
#
#       Boehmke, Bradley. 2016. Data Wrangling with R. New York, NY: Springer Science+Business Media.
#
#       chapter 16  Scrapping data
#
#
#######################################################################################################################
#install.packages("XML")
library(XML)
#install.packages('rvest')
#install.packages("magrittr")
library(rvest)
library(magrittr)
##########################################################################################################################
#
#                               Trying with the NAPT website
#
##########################################################################################################################
##########          Getting the most recent results in the page Laboratory Results   #################################
# Examples of the addres where the pdf files can be downloaded
# http://www.naptprogram.org/files/napt/lab-results/2017-q1-soil-general-report.pdf
#
# http://www.naptprogram.org/files/napt/lab-results/2017-q2-soil-report.pdf
#########     gettting all the date from  http://www.naptprogram.org/lab-results
scraping_NAPT <- read_html ("http://www.naptprogram.org/lab-results")
Lab_Results   <- scraping_NAPT %>%
html_nodes ("td>a")
length(Lab_Results)
Node.names.scraping_NAPT<-html_text(Lab_Results) ;
str(Node.names.scraping_NAPT)
Soils.NAPT<-which(Node.names.scraping_NAPT == "Soil") ;
length(Lab_Results[Soils.NAPT])
head(Lab_Results[Soils.NAPT])
Soil.NAPT.paths<-strsplit(as.character(Lab_Results[Soils.NAPT]), split='"') ;
str(Soil.NAPT.paths)
NAPT.pdfs.1<-sapply(Soil.NAPT.paths,'[',2) ;
NAPT.paths<-paste0('http://www.naptprogram.org/',NAPT.pdfs.1) ;
library(tabulizer)
library(dplyr)
out.1 <- extract_tables(NAPT.paths[1]) ;
str(out.1)
length(out.1)
out.1[[1]]
str(out.1[[1]])
soil.names<-out.1[[1]][1,grep("Soil .*",out.1[[1]][1,])]
temp<-matrix(c(paste(soil.names,c("Median"),sep='_'),paste(soil.names,c("MAD"),sep='_')),nrow=length(soil.names),ncol=2)
Soil_names<-as.vector(t(temp))
analysis.names<-c("Analysis" , "Units" , "n", as.vector(t(temp)))
pdf.data.1<-t(out.1[[length(out.1)]]);
row.names(pdf.data.1)<-analysis.names
str(pdf.data.1)
select.columns<-sort(c(grep('Sand.*',pdf.data.1[1,]),grep('Silt.*',pdf.data.1[1,]),grep('Clay.*',pdf.data.1[1,])));
Results.data.1<-pdf.data.1[,select.columns]
Results.data.all<-Results.data.1
for (i %in% NAPT.paths[2:length(NAPT.paths)])
{
#i=2
########### read the pdf to extract the columns and row names   #################
data.1<-Results.data.all
out<-extract_tables(NAPT.paths[i]) ;
str(out)
length(out)
out[[1]]
str(out[[1]])
soil.names<-out[[1]][1,grep("Soil .*",out[[1]][1,])]
temp<-matrix(c(paste(soil.names,c("Median"),sep='_'),paste(soil.names,c("MAD"),sep='_')),nrow=length(soil.names),ncol=2)
Soil_names<-as.vector(t(temp))
analysis.names<-c("Analysis" , "Units" , "n", as.vector(t(temp)))
############## get the data from the table ###############
pdf.data<-t(out[[length(out)]]);
row.names(pdf.data)<-analysis.names
str(pdf.data)
select.columns<-sort(c(grep('Sand.*',pdf.data[1,]),grep('Silt.*',pdf.data[1,]),grep('Clay.*',pdf.data[1,])));
Results.data<-pdf.data[,select.columns]
############# combine the data with the previous pdf data ###########
str(data.1)
str(Results.data)
Results.data.all<-rbind(data.1,Results.data)
str(Results.data.all)
}
for (i %in% NAPT.paths[2:length(NAPT.paths)]) {
#i=2
########### read the pdf to extract the columns and row names   #################
data.1<-Results.data.all
out<-extract_tables(NAPT.paths[i]) ;
str(out)
length(out)
out[[1]]
str(out[[1]])
soil.names<-out[[1]][1,grep("Soil .*",out[[1]][1,])]
temp<-matrix(c(paste(soil.names,c("Median"),sep='_'),paste(soil.names,c("MAD"),sep='_')),nrow=length(soil.names),ncol=2)
Soil_names<-as.vector(t(temp))
analysis.names<-c("Analysis" , "Units" , "n", as.vector(t(temp)))
############## get the data from the table ###############
pdf.data<-t(out[[length(out)]]);
row.names(pdf.data)<-analysis.names
str(pdf.data)
select.columns<-sort(c(grep('Sand.*',pdf.data[1,]),grep('Silt.*',pdf.data[1,]),grep('Clay.*',pdf.data[1,])));
Results.data<-pdf.data[,select.columns]
############# combine the data with the previous pdf data ###########
str(data.1)
str(Results.data)
Results.data.all<-rbind(data.1,Results.data)
str(Results.data.all)
}
for (i %in% NAPT.paths[2:length(NAPT.paths)]) {
#i=2
########### read the pdf to extract the columns and row names   #################
data.1<-Results.data.all
out<-extract_tables(NAPT.paths[i]) ;
str(out)
length(out)
out[[1]]
str(out[[1]])
soil.names<-out[[1]][1,grep("Soil .*",out[[1]][1,])]
temp<-matrix(c(paste(soil.names,c("Median"),sep='_'),paste(soil.names,c("MAD"),sep='_')),nrow=length(soil.names),ncol=2)
Soil_names<-as.vector(t(temp))
analysis.names<-c("Analysis" , "Units" , "n", as.vector(t(temp)))
############## get the data from the table ###############
pdf.data<-t(out[[length(out)]]);
row.names(pdf.data)<-analysis.names
str(pdf.data)
select.columns<-sort(c(grep('Sand.*',pdf.data[1,]),grep('Silt.*',pdf.data[1,]),grep('Clay.*',pdf.data[1,])));
Results.data<-pdf.data[,select.columns]
############# combine the data with the previous pdf data ###########
str(data.1)
str(Results.data)
Results.data.all<-rbind(data.1,Results.data)
str(Results.data.all)
}
seq(2:length(NAPT.paths)
)
length(NAPT.paths)
seq(2,40)
seq(2:length(NAPT.paths))
seq(2,length(NAPT.paths))
for (i %in% seq(2,length(NAPT.paths)) {
for (i in seq(2,length(NAPT.paths)) {
for (i in seq(2,length(NAPT.paths))) {
#i=2
########### read the pdf to extract the columns and row names   #################
data.1<-Results.data.all
out<-extract_tables(NAPT.paths[i]) ;
str(out)
length(out)
out[[1]]
str(out[[1]])
soil.names<-out[[1]][1,grep("Soil .*",out[[1]][1,])]
temp<-matrix(c(paste(soil.names,c("Median"),sep='_'),paste(soil.names,c("MAD"),sep='_')),nrow=length(soil.names),ncol=2)
Soil_names<-as.vector(t(temp))
analysis.names<-c("Analysis" , "Units" , "n", as.vector(t(temp)))
############## get the data from the table ###############
pdf.data<-t(out[[length(out)]]);
row.names(pdf.data)<-analysis.names
str(pdf.data)
select.columns<-sort(c(grep('Sand.*',pdf.data[1,]),grep('Silt.*',pdf.data[1,]),grep('Clay.*',pdf.data[1,])));
Results.data<-pdf.data[,select.columns]
############# combine the data with the previous pdf data ###########
str(data.1)
str(Results.data)
Results.data.all<-rbind(data.1,Results.data)
str(Results.data.all)
}
Results.data.all
tail(Results.data.all)
download.file('http://www.naptprogram.org/files/napt/publications/program-archive/soils/2005-116-120.pdf')
download.file('http://www.naptprogram.org/files/napt/publications/program-archive/soils/2005-116-120.pdf',"soil.data")
getwd()
download.file('http://www.naptprogram.org/files/napt/publications/program-archive/soils/2005-116-120.pdf',"soil.data.pdf")
download.file('http://www.naptprogram.org/files/napt/publications/program-archive/soils/2005-116-120.pdf',"soil.data.pdf")
download.file('http://www.naptprogram.org/files/napt/publications/program-archive/soils/2005-116-120.pdf',"soil.data")
download.file('http://www.naptprogram.org/files/napt/publications/program-archive/soils/2005-116-120.pdf',"soil_data.pdf", mode="wb")
